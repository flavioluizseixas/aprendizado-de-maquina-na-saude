{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir um ambiente customizado para controle de glicemia\n",
    "class DiabetesEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Definindo o espaço de observação e ação\n",
    "        self.action_space = gym.spaces.Discrete(3)  # [0: não faz nada, 1: tomar insulina, 2: fazer exercício]\n",
    "        self.observation_space = gym.spaces.Discrete(10)  # Exemplo: 10 níveis de glicemia\n",
    "\n",
    "        self.state = 5  # Estado inicial (nível de glicose médio)\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        # Simular mudanças no nível de glicose dependendo da ação\n",
    "        if action == 0:  # Não fazer nada\n",
    "            self.state += np.random.randint(-1, 2)  # Pequenas flutuações\n",
    "        elif action == 1:  # Tomar insulina\n",
    "            self.state -= np.random.randint(1, 3)  # Glicose cai\n",
    "        elif action == 2:  # Fazer exercício\n",
    "            self.state -= np.random.randint(1, 2)  # Glicose cai levemente\n",
    "\n",
    "        # Garantir que o nível de glicose não fique negativo\n",
    "        if self.state < 0:\n",
    "            self.state = 0\n",
    "\n",
    "        # Penalizar níveis de glicose muito altos ou muito baixos\n",
    "        if self.state < 3:\n",
    "            reward = -1  # Glicose muito baixa\n",
    "        elif self.state > 7:\n",
    "            reward = -1  # Glicose muito alta\n",
    "        else:\n",
    "            reward = 1  # Glicose estável\n",
    "\n",
    "        self.done = self.state <= 0 or self.state >= 9  # Episódio termina se glicose estiver fora do intervalo\n",
    "\n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 5  # Resetar o nível de glicose\n",
    "        self.done = False\n",
    "        return self.state, {}\n",
    "\n",
    "def eps_greedy(Q, s, eps=0.1):\n",
    "    if np.random.uniform(0, 1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        return greedy(Q, s)\n",
    "\n",
    "def greedy(Q, s):\n",
    "    return np.argmax(Q[s])\n",
    "\n",
    "def run_episodes(env, Q, num_episodes=100):\n",
    "    tot_rew = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        game_rew = 0\n",
    "        while not done:\n",
    "            next_state, rew, done, _ = env.step(greedy(Q, state))\n",
    "            state = next_state\n",
    "            game_rew += rew\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                tot_rew.append(game_rew)\n",
    "\n",
    "    return np.mean(tot_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q, s, eps=0.1):\n",
    "    if np.random.uniform(0, 1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        return greedy(Q, s)\n",
    "\n",
    "def greedy(Q, s):\n",
    "    return np.argmax(Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(env, Q, num_episodes=100):\n",
    "    tot_rew = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        game_rew = 0\n",
    "        while not done:\n",
    "            next_state, rew, done, _ = env.step(greedy(Q, state))\n",
    "            state = next_state\n",
    "            game_rew += rew\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                tot_rew.append(game_rew)\n",
    "\n",
    "    return np.mean(tot_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.0001):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((nS, nA))\n",
    "    games_reward = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        tot_rew = 0\n",
    "        if eps > 0.01:\n",
    "            eps -= eps_decay\n",
    "\n",
    "        while not done:\n",
    "            action = eps_greedy(Q, state, eps)\n",
    "            next_state, rew, done, _ = env.step(action)\n",
    "\n",
    "            # Atualização do Q-learning\n",
    "            Q[state][action] = Q[state][action] + lr * (rew + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += rew\n",
    "            if done:\n",
    "                games_reward.append(tot_rew)\n",
    "\n",
    "        if (ep % 300) == 0:\n",
    "            test_rew = run_episodes(env, Q, 100)\n",
    "            print(f\"Episode:{ep:5d}  Eps:{eps:.4f}  Rew:{test_rew:.4f}\")\n",
    "            test_rewards.append(test_rew)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:    0  Eps:0.4999  Rew:6.6800\n",
      "Episode:  300  Eps:0.4699  Rew:19.0600\n",
      "Episode:  600  Eps:0.4399  Rew:33.5900\n",
      "Episode:  900  Eps:0.4099  Rew:30.1300\n",
      "Episode: 1200  Eps:0.3799  Rew:33.9400\n",
      "Episode: 1500  Eps:0.3499  Rew:33.6400\n",
      "Episode: 1800  Eps:0.3199  Rew:35.6000\n",
      "Episode: 2100  Eps:0.2899  Rew:39.7000\n",
      "Episode: 2400  Eps:0.2599  Rew:35.8300\n",
      "Episode: 2700  Eps:0.2299  Rew:42.1700\n",
      "Episode: 3000  Eps:0.1999  Rew:40.5400\n",
      "Episode: 3300  Eps:0.1699  Rew:36.3900\n",
      "Episode: 3600  Eps:0.1399  Rew:39.4000\n",
      "Episode: 3900  Eps:0.1099  Rew:33.4900\n",
      "Episode: 4200  Eps:0.0799  Rew:19.6200\n",
      "Episode: 4500  Eps:0.0499  Rew:27.3200\n",
      "Episode: 4800  Eps:0.0199  Rew:38.2500\n",
      "[[ 0.          0.          0.        ]\n",
      " [-1.38262463 -1.         -1.        ]\n",
      " [ 0.33708921 -1.57402799 -1.94729216]\n",
      " [ 4.88045743 -1.10838011  0.25137046]\n",
      " [ 9.38365397  2.0302528   5.87368261]\n",
      " [12.52525146  9.30185604  9.51110331]\n",
      " [13.84780889 11.2573141  12.58669453]\n",
      " [13.09458182 13.35575307 14.20964877]\n",
      " [ 1.61919981 13.70422941  6.74803408]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = DiabetesEnv()\n",
    "    # Definir uma semente para a geração de números aleatórios\n",
    "    np.random.seed(42)\n",
    "    Q_qlearning = Q_learning(env, lr=0.1, num_episodes=5000, eps=0.5, gamma=0.95, eps_decay=0.0001)\n",
    "    print(Q_qlearning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
